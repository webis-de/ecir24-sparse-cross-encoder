{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import re\n",
                "import sys\n",
                "import warnings\n",
                "from collections import defaultdict\n",
                "from dataclasses import asdict, dataclass\n",
                "from pathlib import Path\n",
                "from typing import List\n",
                "\n",
                "import aquarel\n",
                "import ir_datasets\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import trectools\n",
                "import yaml\n",
                "from aquarel import Theme\n",
                "from scipy.stats import pearsonr, ttest_ind, ttest_rel\n",
                "# from tqdm.autonotebook import tqdm\n",
                "from tqdm import tqdm\n",
                "\n",
                "sys.path.append(\"../..\")\n",
                "from sparse_cross_encoder.data.ir_dataset_utils import DASHED_DATASET_MAP, get_base"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# theme = aquarel.Theme.from_file(str(Path.home() / \"aquarel-theme.json\"))\n",
                "# theme.apply()\n",
                "theme = Theme(name=\"theme\").set_grid(draw=True).set_font(family=\"serif\")\n",
                "theme.apply()\n",
                "markers = [\"o\", \"s\", \"v\", \"X\", \"P\", \"*\", \"D\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BASELINE_DIR = Path(\"../../data/baseline-runs\").resolve()\n",
                "LOG_DIR = Path(\"../../experiments/sparse-cross-encoder\").resolve()\n",
                "ARCHIVE_LOG_DIR = Path(\"../../logs/archive\").resolve()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LEVELS = {\n",
                "    0.05: \"*\",\n",
                "    #     0.01: \"**\",\n",
                "    #     0.005: \"***\",\n",
                "}\n",
                "\n",
                "\n",
                "def mean_run_to_full_runs(run, full_run_df):\n",
                "    columns = list(filter(lambda x: \"@\" not in x, run.index))\n",
                "    run = run.loc[columns]\n",
                "    runs = full_run_df.merge(run.to_frame().T, how=\"inner\", on=columns)\n",
                "    return runs\n",
                "\n",
                "\n",
                "def significance(\n",
                "    per_query_df,\n",
                "    baseline_run_name,\n",
                "    comparator_run_names,\n",
                "    metric,\n",
                "    datasets=None,\n",
                "    corpora=None,\n",
                "    bound=1,\n",
                "):\n",
                "    if datasets is not None:\n",
                "        per_query_df = per_query_df.loc[per_query_df[\"dataset\"].isin(datasets)]\n",
                "    elif corpora is not None:\n",
                "        per_query_df = per_query_df.loc[per_query_df[\"base\"].isin(corpora)]\n",
                "    else:\n",
                "        raise ValueError(\"Either dataset or corpus must be specified\")\n",
                "    if pd.isna(baseline_run_name):\n",
                "        return [np.nan] * len(comparator_run_names)\n",
                "    per_query_df = per_query_df.set_index(\"run_name\")[metric]\n",
                "    if baseline_run_name not in per_query_df.index:\n",
                "        return [np.nan] * len(comparator_run_names)\n",
                "    baseline_run = per_query_df.loc[baseline_run_name].fillna(0)\n",
                "    results = []\n",
                "    for comparator_run_name in comparator_run_names:\n",
                "        if comparator_run_name == baseline_run_name:\n",
                "            results.append(np.nan)\n",
                "            continue\n",
                "        if pd.isna(comparator_run_name):\n",
                "            results.append(np.nan)\n",
                "            continue\n",
                "        comparator_run = per_query_df.loc[comparator_run_name].fillna(0)\n",
                "        warnings.filterwarnings(\"error\")\n",
                "        if bound:\n",
                "            try:\n",
                "                p_greater = ttest_rel(\n",
                "                    baseline_run.values + bound, comparator_run.values, alternative=\"greater\"\n",
                "                )[1]\n",
                "                p_less = ttest_rel(\n",
                "                    baseline_run.values - bound, comparator_run.values, alternative=\"less\"\n",
                "                )[1]\n",
                "                p_value = max(p_greater, p_less)\n",
                "            except RuntimeWarning:\n",
                "                p_value = 0\n",
                "        else:\n",
                "            p_value = ttest_rel(baseline_run.values, comparator_run.values)[1]\n",
                "        warnings.resetwarnings()\n",
                "        results.append(p_value)\n",
                "        # star = \"\"\n",
                "        # for level, stars in LEVELS.items():\n",
                "        #     if p_value < level:\n",
                "        #         star = stars\n",
                "        # results.append((p_value, star))\n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_config(config_path):\n",
                "    config = yaml.safe_load(config_path.read_text())\n",
                "\n",
                "    flat_config = {}\n",
                "\n",
                "    def flatten_config(config, prefix=\"\"):\n",
                "        for k, v in config.items():\n",
                "            key = prefix + \".\" + k if prefix else k\n",
                "            if isinstance(v, dict):\n",
                "                if len(v) == 2 and \"init_args\" in v:\n",
                "                    flatten_config(v[\"init_args\"], key)\n",
                "                else:\n",
                "                    flatten_config(v, key)\n",
                "            else:\n",
                "                flat_config[key] = v\n",
                "        return config\n",
                "\n",
                "    flatten_config(config)\n",
                "    for k, v in list(flat_config.items()):\n",
                "        if hasattr(v, \"__iter__\"):\n",
                "            del flat_config[k]\n",
                "    return flat_config\n",
                "\n",
                "\n",
                "def load_run(run_file):\n",
                "    run = pd.read_csv(\n",
                "        run_file,\n",
                "        sep=r\"\\s+\",\n",
                "        header=None,\n",
                "        names=[\"query_id\", \"Q0\", \"doc_id\", \"rank\", \"score\", \"run_name\"],\n",
                "    )\n",
                "    config = {}\n",
                "    config_path = run_file.parent.parent / \"pl_config.yaml\"\n",
                "    if \"bm25\" in run.iloc[0][\"run_name\"].lower():\n",
                "        run[\"run_name\"] = \"bm25\"\n",
                "    if config_path.exists():\n",
                "        run_id = run_file.parent.parent.parent.name[20:]\n",
                "        config = parse_config(config_path)\n",
                "        run[\"run_name\"] = run_id\n",
                "    # if arguana or quora, remove queries from run\n",
                "    # https://twitter.com/nandan__thakur/status/1603920955679551488\n",
                "    # https://github.com/beir-cellar/beir/blob/bc4d2b50b0059c0895282b609ff30b0530ed6648/beir/retrieval/evaluation.py#L49\n",
                "    dataset = DASHED_DATASET_MAP[run_file.stem]\n",
                "    if dataset.startswith(\"beir/arguana\") or dataset.startswith(\"beir/quora\"):\n",
                "        run = run.loc[run[\"query_id\"] != run[\"doc_id\"]]\n",
                "        run[\"rank\"] = run.groupby(\"query_id\")[\"score\"].rank(ascending=False).astype(int)\n",
                "    config[\"dataset\"] = dataset\n",
                "    config[\"base\"] = get_base(dataset)\n",
                "    if \"medline\" in config[\"base\"]:\n",
                "        config[\"base\"] = \"medline\"\n",
                "    config[\"run_name\"] = run.iloc[0][\"run_name\"]\n",
                "    run = run.astype({\"query_id\": str, \"doc_id\": str})\n",
                "    return run, config\n",
                "\n",
                "\n",
                "def load_qrels(dataset):\n",
                "    qrels = trectools.TrecQrel()\n",
                "    qrels_df = pd.DataFrame(ir_datasets.load(dataset).qrels_iter())\n",
                "    qrels_df = qrels_df.rename(\n",
                "        {\"query_id\": \"query\", \"doc_id\": \"docid\", \"relevance\": \"rel\", \"iteration\": \"q0\"},\n",
                "        axis=1,\n",
                "    )\n",
                "    qrels.qrels_data = qrels_df\n",
                "    return qrels\n",
                "\n",
                "\n",
                "def evaluate_run(run_df, qrels, metrics, metric_kwargs):\n",
                "    metric_to_func = {\n",
                "        \"NDCG\": \"get_ndcg\",\n",
                "        \"recip_rank\": \"get_reciprocal_rank\",\n",
                "        \"UNJ\": \"get_unjudged\",\n",
                "    }\n",
                "    run_df = run_df.rename(\n",
                "        {\"query_id\": \"query\", \"Q0\": \"q0\", \"doc_id\": \"docid\", \"run_name\": \"system\"},\n",
                "        axis=1,\n",
                "    )\n",
                "    run = trectools.TrecRun()\n",
                "    run.run_data = run_df\n",
                "    trec_eval = trectools.TrecEval(run, qrels)\n",
                "    metric_dfs = []\n",
                "    for full_metric, kwargs in zip(metrics, metric_kwargs):\n",
                "        metric, depth = full_metric.split(\"@\")\n",
                "        depth = depth.split(\"_\")[0]\n",
                "        depth = int(depth)\n",
                "        values = getattr(trec_eval, metric_to_func[metric])(\n",
                "            depth, per_query=True, **kwargs\n",
                "        )\n",
                "        values = values.rename(lambda x: full_metric, axis=1)\n",
                "        metric_dfs.append(values.fillna(0))\n",
                "    metric_df = pd.concat(metric_dfs, axis=1)\n",
                "    return metric_df\n",
                "\n",
                "\n",
                "def concat_df_config(df, config):\n",
                "    length = df.shape[1] + len(config)\n",
                "    values = np.empty((df.shape[0], length), dtype=\"object\")\n",
                "    values[:, : df.shape[1]] = df.values\n",
                "    values[:, df.shape[1] :] = np.array(list(config.values()))\n",
                "    columns = list(df.columns) + list(config.keys())\n",
                "    return pd.DataFrame(values, columns=columns)\n",
                "\n",
                "def get_query_id_doc_id_pairs(df):\n",
                "    return df.loc[:, [\"query_id\", \"doc_id\"]].apply(tuple, axis=1)\n",
                "\n",
                "def clean_run_df(df, bm25_df):\n",
                "    df = df.loc[\n",
                "        get_query_id_doc_id_pairs(df).isin(get_query_id_doc_id_pairs(bm25_df))\n",
                "    ].copy()\n",
                "    df[\"rank\"] = df.groupby(\"query_id\")[\"score\"].rank(ascending=False).astype(int)\n",
                "    return df\n",
                "\n",
                "def parse_run_type(df):\n",
                "    run_type = pd.Series(index=df.index, dtype=\"object\")\n",
                "    run_type.loc[df[\"model.config.query_doc_attention\"].fillna(False)] = \"full attention\"\n",
                "    run_type.loc[df[\"model.qds_transformer\"].fillna(False)] = \"qds transformer\"\n",
                "    run_type.loc[~df[\"model.config.query_doc_attention\"].fillna(False)] = \"sparse cross encoder\"\n",
                "    run_type.loc[df[\"run_name\"].str.lower().str.contains(\"bm25\")] = \"bm25\"\n",
                "    run_type.loc[df[\"run_name\"].str.lower().str.contains(\"colbert\")] = \"colbert\"\n",
                "    return run_type\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "depth = 100\n",
                "metrics = [\"NDCG@10\", \"NDCG@10_UNJ\", f\"recip_rank@{depth}\", \"UNJ@10\"]\n",
                "metric_kwargs = [{}, {\"removeUnjudged\": True}, {}, {}]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "per_query_dfs = []\n",
                "\n",
                "run_files = (\n",
                "    list(BASELINE_DIR.glob(\"bm25/*.run\"))\n",
                "    # + list(BASELINE_DIR.glob(\"colbert/*.run\"))\n",
                "    + list(LOG_DIR.glob(\"**/*.run\"))\n",
                "    # + list(ARCHIVE_LOG_DIR.glob(\"run-*/**/*.run\"))\n",
                ")\n",
                "bm25_runs = {}\n",
                "\n",
                "pg = tqdm(run_files)\n",
                "for run_file in pg:\n",
                "    pg.set_description(run_file.name)\n",
                "    if \"trec-dl\" not in str(run_file):\n",
                "        continue\n",
                "    # if (\n",
                "    #     \"beir\" in str(run_file)\n",
                "    #     or \"tripclick\" in str(run_file)\n",
                "    #     or \"orcas\" in str(run_file)\n",
                "    #     or \"msmarco-passage-v2\" in str(run_file)\n",
                "    #     or \"train\" in str(run_file)\n",
                "    #     or \"dev\" in str(run_file)\n",
                "    # ):\n",
                "    #     continue\n",
                "    # try:\n",
                "    run_df, config = load_run(run_file)\n",
                "    # except:\n",
                "    #     continue\n",
                "    # if \"bm25\" in str(run_file):\n",
                "    #     run_df = run_df.groupby(\"query_id\").head(depth)\n",
                "    #     bm25_runs[config[\"dataset\"]] = run_df\n",
                "    # else:\n",
                "    #     run_df = clean_run_df(run_df, bm25_runs[config[\"dataset\"]])\n",
                "    qrels = load_qrels(config[\"dataset\"])\n",
                "    metric_df = evaluate_run(run_df, qrels, metrics, metric_kwargs)\n",
                "    eval_df = concat_df_config(metric_df.reset_index().astype({\"query\": str}), config)\n",
                "    per_query_dfs.append(eval_df)\n",
                "\n",
                "per_query_results = pd.concat(per_query_dfs).infer_objects().reset_index(drop=True)\n",
                "per_query_results[\"run_type\"] = parse_run_type(per_query_results)\n",
                "\n",
                "# all_query_results = per_query_results.copy()\n",
                "# all_query_results[\"base\"] = \"all\"\n",
                "# all_query_results[\"dataset\"] = \"all\"\n",
                "\n",
                "passage_query_results = per_query_results.copy().loc[\n",
                "    per_query_results.dataset.str.contains(\"passage\")\n",
                "]\n",
                "passage_query_results[\"base\"] = \"passage\"\n",
                "passage_query_results[\"dataset\"] = \"passage\"\n",
                "\n",
                "document_query_results = per_query_results.copy().loc[\n",
                "    per_query_results.dataset.str.contains(\"document\")\n",
                "]\n",
                "document_query_results[\"base\"] = \"document\"\n",
                "document_query_results[\"dataset\"] = \"document\"\n",
                "\n",
                "per_query_results = pd.concat(\n",
                "    [\n",
                "        per_query_results,\n",
                "        # all_query_results,\n",
                "        passage_query_results,\n",
                "        document_query_results,\n",
                "    ]\n",
                ")\n",
                "\n",
                "del per_query_dfs\n",
                "# per_query_results.loc[\n",
                "#     per_query_results[\"dataset\"].str.startswith(\"beir/cqadupstack\"), \"dataset\"\n",
                "# ] = \"beir/cqadupstack\"\n",
                "per_query_results.reset_index().to_json(\"per_query_results.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "per_query_results = pd.read_json(\"per_query_results.json\").reset_index(drop=True)\n",
                "per_query_results = per_query_results.drop(\"index\", axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
                "groupby_columns = list(\n",
                "    filter(lambda x: x not in metrics + [\"query\"], per_query_results.columns)\n",
                ")\n",
                "results = (\n",
                "    per_query_results.groupby(groupby_columns, dropna=False)[metrics]\n",
                "    .mean()\n",
                "    .reset_index()\n",
                "    .copy()\n",
                ")\n",
                "groupby_columns = list(\n",
                "    filter(lambda x: x not in metrics + [\"dataset\", \"query\"], per_query_results.columns)\n",
                ")\n",
                "base_results = (\n",
                "    per_query_results.groupby(groupby_columns, dropna=False)[metrics].mean().reset_index().copy()\n",
                ")\n",
                "groupby_columns = list(\n",
                "    filter(lambda x: x not in metrics + [\"dataset\", \"query\", \"base\"], per_query_results.columns)\n",
                ")\n",
                "base_results.head(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "per_query_results.pivot(\n",
                "    index=[\n",
                "        \"model.config.attention_window_size\",\n",
                "        \"model.config.query_cls_attention\",\n",
                "        \"model.config.cls_query_attention\",\n",
                "        \"model.config.doc_query_attention\",\n",
                "        \"run_name\",\n",
                "    ],\n",
                "    columns=[\"query\", \"dataset\"],\n",
                "    values=[\"NDCG@10\"],\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results.pivot(\n",
                "    index=[\n",
                "        \"model.config.attention_window_size\",\n",
                "        \"model.config.query_cls_attention\",\n",
                "        \"run_name\",\n",
                "    ],\n",
                "    columns=[\"dataset\"],\n",
                "    values=[\"NDCG@10\"],\n",
                ").multiply(100).round(1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results.pivot(\n",
                "    index=[\n",
                "        \"run_type\",\n",
                "        \"model.config.attention_window_size\",\n",
                "        # \"model.config.max_position_embeddings\",\n",
                "        # \"model.config.query_cls_attention\",\n",
                "        # \"model.config.doc_query_attention\",\n",
                "        \"run_name\",\n",
                "    ],\n",
                "    columns=[\"dataset\"],\n",
                "    values=[\"NDCG@10\"],\n",
                ").multiply(100).round(1).transpose()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_results.pivot(\n",
                "    index=[\n",
                "        \"run_type\",\n",
                "        \"model.config.max_position_embeddings\",\n",
                "        # \"model.config.query_cls_attention\",\n",
                "        # \"model.config.doc_query_attention\",\n",
                "        \"model.config.attention_window_size\",\n",
                "        \"run_name\"\n",
                "    ],\n",
                "    columns=\"base\",\n",
                "    values=[\"NDCG@10\"],\n",
                ").round(3) * 100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base = False\n",
                "if base:\n",
                "    table = base_results.copy()\n",
                "else:\n",
                "    table = results.copy()\n",
                "table[\"model.config.attention_window_size\"] = (\n",
                "    table[\"model.config.attention_window_size\"].fillna(float(\"inf\")).copy()\n",
                ")\n",
                "table = table.pivot(\n",
                "    index=[\n",
                "        \"run_type\",\n",
                "        \"model.config.attention_window_size\",\n",
                "    ],\n",
                "    columns=[\"base\" if base else \"dataset\"],\n",
                "    values=[\"NDCG@10\", \"run_name\"],\n",
                ")\n",
                "# table = table.drop(\"colbert\", level=\"run_type\")\n",
                "table = table.sort_index(ascending=(True, False, True))\n",
                "\n",
                "table\n",
                "run_name_table = table.loc[:, \"run_name\"]\n",
                "table = table.loc[:, \"NDCG@10\"]\n",
                "\n",
                "tost = []\n",
                "ttest = []\n",
                "for corpus in table.columns:\n",
                "    # baseline = run_name_table.loc[(\"full attention\", 512, float(\"inf\")), corpus]\n",
                "    if \"passage\" in corpus:\n",
                "        baseline = \"g94mcy7f\"\n",
                "    else:\n",
                "        baseline = \"3u6n318u\"\n",
                "    if isinstance(baseline, pd.Series):\n",
                "        baseline = baseline.iloc[0]\n",
                "    run_names = run_name_table.loc[:, corpus].values.tolist()\n",
                "    corpus_sig_results = []\n",
                "    if corpus == \"out-of-domain\":\n",
                "        corpora = table.index[table.index.values != \"msmarco-passage\"].values\n",
                "    else:\n",
                "        corpora = [corpus]\n",
                "    key = \"corpora\" if base else \"datasets\"\n",
                "    kwargs = {key: corpora}\n",
                "    tost.append(\n",
                "        significance(\n",
                "            per_query_results,\n",
                "            baseline,\n",
                "            run_names,\n",
                "            \"NDCG@10\",\n",
                "            bound=0.02,\n",
                "            **kwargs,\n",
                "        )\n",
                "    )\n",
                "    ttest.append(\n",
                "        significance(\n",
                "            per_query_results,\n",
                "            baseline,\n",
                "            run_names,\n",
                "            \"NDCG@10\",\n",
                "            bound=0,\n",
                "            **kwargs,\n",
                "        )\n",
                "    )\n",
                "tost_df = pd.DataFrame(tost, index=table.columns, columns=table.index)\n",
                "# ttest_df = pd.DataFrame(ttest, index=table.columns, columns=table.index)\n",
                "\n",
                "\n",
                "def format_row(series, tost_df):\n",
                "    rounded = (series * 100).fillna(0).round(1)\n",
                "    max_val = rounded.max()\n",
                "    tost_row = tost_df.loc[series.name, series.index]\n",
                "    out_values = []\n",
                "    iterator = zip(tost_row, rounded)\n",
                "    for tost_p_val, val in iterator:\n",
                "        out_val = f\"{val:.1f}\"\n",
                "        if out_val.startswith(\"0.\"):\n",
                "            out_val = \"0\" + out_val\n",
                "        if val == max_val:\n",
                "            out_val = \"\\\\textbf{\" + out_val + \"}\"\n",
                "        if tost_p_val < 0.05 / (series.shape[0] - 1):\n",
                "            out_val = out_val + \"\\\\kernSigtost\"\n",
                "        out_values.append(out_val)\n",
                "    out = pd.Series(out_values, index=series.index)\n",
                "    return out\n",
                "\n",
                "\n",
                "table = table.loc[\n",
                "    [\"full attention\", \"sparse cross encoder\", \"qds transformer\"]\n",
                "].transpose()\n",
                "\n",
                "table.fillna(0).multiply(100).round(1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pretty_table = pd.concat(\n",
                "    [\n",
                "        table.filter(like=\"passage\", axis=0).apply(\n",
                "            lambda x: (format_row(x, tost_df)), axis=1\n",
                "        ),\n",
                "        table.filter(like=\"document\", axis=0).apply(\n",
                "            lambda x: (format_row(x, tost_df)), axis=1\n",
                "        ),\n",
                "    ]\n",
                ").loc[\n",
                "    [\n",
                "        \"msmarco-passage/trec-dl-2019/judged\",\n",
                "        \"msmarco-passage/trec-dl-2020/judged\",\n",
                "        \"msmarco-passage-v2/trec-dl-2021/judged\",\n",
                "        \"msmarco-passage-v2/trec-dl-2022/judged\",\n",
                "        \"passage\",\n",
                "        \"msmarco-document/trec-dl-2019/judged\",\n",
                "        \"msmarco-document/trec-dl-2020/judged\",\n",
                "        \"msmarco-document-v2/trec-dl-2021/judged\",\n",
                "        \"msmarco-document-v2/trec-dl-2022/judged\",\n",
                "        \"document\",\n",
                "    ]\n",
                "]\n",
                "print(pretty_table.to_latex(escape=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "idx = pd.IndexSlice\n",
                "# (table.loc[idx[:, :, :, [\"g94mcy7f\", \"p4v9923r\", \"cg4a0ke7\", \"uatdxcst\", \"zzcodw0f\", \"y5pcbt5n\"]],]\n",
                "#     .transpose()\n",
                "#     .filter(like=\"document\", axis=0))\n",
                "print(\n",
                "    table.loc[idx[:, :, :, [\"bm25\", \"0gyv091s\", \"g94mcy7f\", \"p4v9923r\", \"cg4a0ke7\", \"tmj1empz\", \"y5pcbt5n\"]],]\n",
                "    .transpose()\n",
                "    .filter(like=\"document\", axis=0)\n",
                "    .apply(lambda x: (format_row(x, False)), axis=1)\n",
                "    .to_latex(escape=False)\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "datasets = [\"msmarco-passage\", \"out-of-domain\"]\n",
                "dataset_name_map = {\"msmarco-passage\": \"MS MARCO\", \"out-of-domain\": \"Out-of-Domain\"}\n",
                "fig, ax = plt.subplots(1, 2, figsize=(6, 2.9))\n",
                "plot_data = table.loc[datasets].drop(\"Baseline\", axis=1)\n",
                "approaches = plot_data.columns.get_level_values(\"model_name\").unique().tolist()\n",
                "approaches.remove(\"CLS Interaction\")\n",
                "\n",
                "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
                "\n",
                "for ax_idx, dataset in enumerate(datasets):\n",
                "    ax[ax_idx].set_axisbelow(True)\n",
                "    dataset_data = plot_data.loc[dataset]\n",
                "    dataset_data = dataset_data.droplevel(\"run_name\")\n",
                "    ax[ax_idx].set_title(dataset_name_map[dataset])\n",
                "    ax[ax_idx].set_xlabel(\"Attention Window Size\")\n",
                "    ax[ax_idx].set_ylabel(\"NDCG@10\")\n",
                "    attention_window_sizes = set(\n",
                "        dataset_data.index.get_level_values(\"model.config.attention_window_size\").values\n",
                "    )\n",
                "    min_window_size = min(attention_window_sizes - {float(\"inf\")})\n",
                "    max_window_size = max(attention_window_sizes - {float(\"inf\")})\n",
                "    for approach_idx, approach in enumerate(approaches):\n",
                "        color = colors[approach_idx]\n",
                "        approach_data = dataset_data.loc[approach]\n",
                "        no_window = approach_data.loc[float(\"inf\")]\n",
                "        baseline_label = approach.replace(\"/ Longformer\", \"\")\n",
                "        if baseline_label == \"Independent Query\":\n",
                "            baseline_label = \"Independent Query (w=$\\infty$)\"\n",
                "        line = ax[ax_idx].plot(\n",
                "            approach_data.index.values,\n",
                "            approach_data.values,\n",
                "            label=approach.replace(\"Full Attention / \", \"\") if ax_idx == 0 else None,\n",
                "            marker=markers[approach_idx],\n",
                "            color=color,\n",
                "        )\n",
                "        ax[ax_idx].plot(\n",
                "            [min_window_size, max_window_size],\n",
                "            [no_window, no_window],\n",
                "            color=color,\n",
                "            label=baseline_label if ax_idx == 0 else None,\n",
                "            linestyle=\"--\" if approach_idx == 0 else \":\",\n",
                "            # marker=markers[approach_idx],\n",
                "            # w=2,\n",
                "            \n",
                "        )\n",
                "    for approach_idx, approach in enumerate(approaches):\n",
                "        approach_data = dataset_data.loc[approach]\n",
                "        approach_data = approach_data.loc[\n",
                "            approach_data.index.get_level_values(\"model.config.attention_window_size\")\n",
                "            != float(\"inf\")\n",
                "        ]\n",
                "        color = colors[approach_idx]\n",
                "        line = ax[ax_idx].plot(\n",
                "            approach_data.index.values,\n",
                "            approach_data.values,\n",
                "            marker=markers[approach_idx],\n",
                "            color=color,\n",
                "        )\n",
                "# fig.legend(loc=\"center\", bbox_to_anchor=(1.15, 0.5))\n",
                "fig.legend(ncols=2, bbox_to_anchor=(0.5, -0.05), loc=\"center\")\n",
                "fig.tight_layout()\n",
                "plt.savefig(\"domain-effectiveness.pdf\", bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_results.loc[base_results[\"run_name\"].isin((\"hfclj9k8\", \"vtwk7mqt\"))].filter(regex=r\"run_name|model\\.config\\..*_attention$\", axis=1).drop_duplicates()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_results.pivot(\n",
                "    index=[\n",
                "        \"model.config.attention_window_size\",\n",
                "        \"model.config.query_cls_attention\",\n",
                "        \"model.config.cls_query_attention\",\n",
                "        \"model.config.doc_query_attention\",\n",
                "        \"model.config.query_doc_embedding_attention\",\n",
                "        \"run_name\",\n",
                "    ],\n",
                "    columns=\"base\",\n",
                "    values=[\"NDCG@10_UNJ\"],\n",
                ").filter(items=list(set(base_results[\"base\"]) - set(\"msmarco-passage\")))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
